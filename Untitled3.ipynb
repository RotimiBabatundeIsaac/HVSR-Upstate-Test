{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c499db21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/birotimi/Library/CloudStorage/OneDrive-SyracuseUniversity/Desktop/PHD/HVSR-project/HVSR-master'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b20539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    " NAME: computeHVSR.py\n",
    "\n",
    " DESCRIPTION: a Python script that uses IRIS DMC's MUSTANG noise-psd/pdf web services  to compute horizontal-to-vertical\n",
    " spectral ratio (HVSR)\n",
    "\n",
    " Copyright (C) 2019  Product Team, IRIS Data Management Center\n",
    "\n",
    "    This is a free software; you can redistribute it and/or modify\n",
    "    it under the terms of the GNU Lesser General Public License as\n",
    "    published by the Free Software Foundation; either version 3 of the\n",
    "    License, or (at your option) any later version.\n",
    "\n",
    "    This script is distributed in the hope that it will be useful, but\n",
    "    WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n",
    "    Lesser General Public License (GNU-LGPL) for more details.  The\n",
    "    GNU-LGPL and further information can be found here:\n",
    "    http://www.gnu.org/\n",
    "\n",
    "    You should have received a copy of the GNU Affero General Public License\n",
    "    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n",
    "\n",
    " INPUTS: Script expects a configuration parameter file under the 'param' directory. the parameter file's name is the\n",
    " same as this scripts name with '_param' appended.\n",
    " If removing the outliers option is selected, then script also expects a station baseline file as generated by\n",
    " getStationChannelBaseline.py script of this bundle.\n",
    "\n",
    "OUTPUTS:\n",
    "    - HVSR values\n",
    "    - HVSR plots\n",
    "    - SESAME 2004 parameters\n",
    "\n",
    " USAGE:\n",
    "\n",
    " getStationChannelBaseline.py net=netName sta=staName loc=locCode {chan=chanCode} start=2007-03-19\n",
    " end=2008-10-28 {plot=[0, 1] verbose=[0, 1] percentlow=[10] percenthigh=[90] x_type=[period,frequency]}\n",
    "\n",
    " getStationChannelBaseline.py net=IU sta=ANMO loc=00 chan=BHZ start=2002-11-20 end=2008-11-20 plot=1\n",
    " verbose=1 percentlow=10 percenthigh=90\n",
    "\n",
    " the default values for the parameters between {} may be provided in the parameter file\n",
    "\n",
    " HISTORY:\n",
    "    2020-08-27 IRIS DMC Product Team (Manoch): V.2020.240, fixed channel sorting order for horizontal 1 &2 directions.\n",
    "    2020-06-04 IRIS DMC Product Team (Manoch): V.2020.156, addressed the check mark character display issue on Windows.\n",
    "    2020-06-03 IRIS DMC Product Team (Manoch): V.2020.155, addressed the UTF-8 character issue on Windows.\n",
    "    2020-02-24 IRIS DMC Product Team (Manoch): V.2020.055, now the peak report is also writen to a file under the\n",
    "                                               data/report directory.\n",
    "    2019-06-19 IRIS DMC Product Team (Manoch): V.2019.171, added Peterson 1993 NLNM and NHNM to the plots and updated\n",
    "                                               color scale.\n",
    "    2019-06-03 IRIS DMC Product Team (Manoch): V.2019.154, updated the code style\n",
    "    2018-07-10 IRIS DMC Product Team (Manoch): pre-release version V.2018.191\n",
    "    2018-06-18 IRIS DMC Product Team (Manoch): added remove_outliers parameter to allow HVSR computation without\n",
    "                                               removing outliers and added method parameter that the method to use for\n",
    "                                               combining h1 and h2, including DFA V.2018.169\n",
    "    2017-11-28 IRIS DMC Product Team (Manoch): V.2017.332\n",
    "    2017-05-21 IRIS DMC Product Team (Manoch): V.2017.141\n",
    "    2017-03-12 IRIS DMC Product Team (Manoch): created V.2017.071\n",
    "\n",
    " REFERENCES:\n",
    "   Albarello, Dario & Lunedei, Enrico. (2013). Combining horizontal ambient vibration components for H/V spectral ratio\n",
    "              estimates. Geophysical Journal International. 194. 936-951. 10.1093/gji/ggt130.\n",
    "\n",
    "   Francisco J Sanchez-Sesma, Francisco & Rodriguez, Miguel & Iturraran-Viveros, Ursula & Luzn, Francisco & Campillo,\n",
    "              Michel & Margerin, Ludovic & Garcia-Jerez, Antonio & Suarez, Martha & Santoyo, Miguel &\n",
    "              Rodriguez-Castellanos, A. (2011). A theory for microtremor H/V spectral ratio: Application for a\n",
    "              layered medium. Geophysical Journal International. 186. 221-225. 10.1111/j.1365-246X.2011.05064.x.\n",
    "\n",
    "   Guidelines for the Implementation of the H/V Spectral Ratio Technique on Ambient Vibrations, December 2004\n",
    "              SESAME European research project WP12 - Deliverable D23.12, European Commission - Research General\n",
    "              Directorate Project No. EVG1-CT-2000-00026 SESAME.\n",
    "              ftp://ftp.geo.uib.no/pub/seismo/SOFTWARE/SESAME/USER-GUIDELINES/SESAME-HV-User-Guidelines.pdf\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "version = 'V.2020.155'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "import math\n",
    "\n",
    "import time\n",
    "import urllib\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "\n",
    "# Import the HVSR parameters and libraries.\n",
    "hvsrDirectory = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "\n",
    "paramPath = os.path.join(hvsrDirectory, 'param')\n",
    "libPath = os.path.join(hvsrDirectory, 'lib')\n",
    "\n",
    "sys.path.append(paramPath)\n",
    "sys.path.append(libPath)\n",
    "\n",
    "import fileLib as fileLib\n",
    "import msgLib as msgLib\n",
    "import computeHVSR_param as param\n",
    "\n",
    "script = os.path.basename(__file__)\n",
    "\n",
    "greek_chars = {'sigma': u'\\u03C3', 'epsilon': u'\\u03B5', 'teta': u'\\u03B8'}\n",
    "channel_order = {'Z': 0, '1': 1, 'N': 1, '2': 2, 'E': 2}\n",
    "separator_character = '='\n",
    "\n",
    "t0 = time.time()\n",
    "display = True\n",
    "max_rank = 0\n",
    "plotRows = 4\n",
    "\n",
    "\n",
    "def usage():\n",
    "    \"\"\"The usage message.\n",
    "    \"\"\"\n",
    "    print('\\n\\n{} ({}):'.format(script, version))\n",
    "    print('\\nThis script uses IRIS DMC\\'s MUSTANG noise-pdf web service (http://service.iris.edu/mustang/) to\\n'\n",
    "          'compute  horizontal-to-vertical spectral ratio (HVSR) and obtain the predominant frequency at the station\\n'\n",
    "          'site. The amplitude of the HVSR curves represent the ratio of the smoothed amplitude spectra for the \\n'\n",
    "          'horizontal and vertical components as obtained from PSDs. Script provides the following options:\\n'\n",
    "          '  - Remove PSDs that fall outside the station noise baseline, as computed by \\n'\n",
    "          '    computeStationChannelBaseline.py script (parameter removeoutliers=0|1)\\n'\n",
    "          '  - Compute HVSR using one of the methods below (parameter: method=1|2|3|4|5|6)\\n'\n",
    "          '    For a review o0f methods 2-6 see Albarello and Lunedei (2013). \\n'\n",
    "          '\\t\\t(1) DFA, Diffuse Field Assumption method (Sanchez-Sesma et al., 2011)\\n'\n",
    "          '\\n\\t\\tNOTE: The MUSTANG noise-psd web service Power Spectral Density estimate for seismic channels are\\n'\n",
    "          '\\t\\t      computed using the algorithm outlined at:\\n'\n",
    "          '\\t\\t      http://service.iris.edu/mustang/noise-psd/docs/1/help/\\n'\n",
    "          '\\t\\t      This algorithm involves averaging and normalization that may result in smoothing of some of \\n'\n",
    "          '\\t\\t      the peaks that may otherwise be observed by direct computation of FFT and DFA. The peaks\\n'\n",
    "          '\\t\\t      are used  by this script for the predominant frequency determination only.\\n\\n'\n",
    "          '\\t\\t(2) arithmetic mean, (HN + HE)/2\\n'\n",
    "          '\\t\\t(3) geometric mean, sqrt(HN x HE)\\n'\n",
    "          '\\t\\t(4) vector summation, sqrt(HN^2 + HE^2)\\n'\n",
    "          '\\t\\t(5) quadratic mean, sqrt(HN^2 + HE^2)/2\\n'\n",
    "          '\\t\\t(6) maximum horizontal value, max {HN, HE}\\n'\n",
    "          '  - Output a peak rank report with ranking based on SESAME 2004 (modified and not available for DFA method'\n",
    "          ')\\n\\n')\n",
    "    print('\\n\\nUsage:\\n{} net=netName sta=staName loc=locCode chan=chanCodes start=2013-01-01 end=2013-01-01\\n'\n",
    "          'plot=[0, 1] plotbad=[0|1] plotpsd=[0|1] plotpdf=[0|1] plotnnm=[0|1] verbose=[0|1] ymax=[maximum Y value]\\n'\n",
    "          'xtype=[frequency|period] n=[number of segments] removeoutliers=[0|1] method=[1-6] showplot=[0|1]'\n",
    "          .format(script))\n",
    "    print('\\nnet\\t\\tstation network name'\n",
    "          '\\nsta\\t\\tstation name'\n",
    "          '\\nloc\\t\\tstation location code'\n",
    "          '\\nchan\\t\\tstation channel code (separate multiple channel codes by comma); \\n\\t\\tdefault: {}'\n",
    "          '\\nxtype\\t\\tX-axis  type; default: {}'\n",
    "          '\\nstart\\t\\tstart date of the interval for which HVSR to be computed (format YYYY-MM-DD).'\n",
    "          '\\n\\t\\tThe start day begins at 00:00:00 UTC'\n",
    "          '\\nend\\t\\tend date of the interval for which station baseline is computed (format YYYY-MM-DD).'\n",
    "          '\\n\\t\\tThe end day ends at 23:59:59 UTC'\n",
    "          '\\n\\n\\t\\tNOTE: PSD segments will be limited to those starting between start (inclusive) and '\n",
    "          '\\n\\t\\tend (exclusive) except when start and end are the same (in that case, the range will '\n",
    "          '\\n\\t\\tcover start day only).'\n",
    "          '\\n\\nverbose\\t\\tRun in verbose mode to provide informative messages [0=no, 1=yes];'\n",
    "          '\\n\\t\\tdefault:{}'\n",
    "          '\\nplotbad\\t\\tplot rejected PSDs (float) if \"plotpsd\" option is selected; default {}'\n",
    "          '\\nplotnnm\\t\\tplot the New Noise Models [0|1], active if plot=1; default {}'\n",
    "          '\\nplotpsd\\t\\tplot PSDs; default {}'\n",
    "          '\\nplotpdf\\t\\tplot PSD\\\\DFs; default {}'\n",
    "          '\\nymax\\t\\tmaximum Y values; default {}'\n",
    "          '\\nn\\t\\tbreak start-end interval into \\'n\\' segments; default {}'\n",
    "          '\\nremoveoutliers\\tremove PSDs that fall outside the station noise baseline; default {}'\n",
    "          '\\nymax\\t\\tmcompute HVSR using method (see above); default {}'\n",
    "          '\\nshowplot\\tturn plot display on/off default is {} (plot file is generated for both options)'\n",
    "          .format(param.chan, param.xtype, param.verbose, param.plotbad, param.plotnnm, param.plotpsd, param.plotpdf,\n",
    "                  param.yLim[1],\n",
    "                  param.n, param.removeoutliers, param.method, param.showplot))\n",
    "    print('\\n\\nExamples:'\n",
    "          '\\n{} net=TA sta=TCOL loc=-- chan=BHZ,BHN,BHE start=2013-01-01 end=2013-01-01 plot=1 plotbad=0 '\n",
    "          'plotpsd=0 plotpdf=1 verbose=1 ymax=5 xtype=frequency n=1 removeoutliers=0 method=4'.format(script))\n",
    "    print('\\n{} net=TA sta=TCOL loc=-- chan=BHZ,BHN,BHE start=2013-01-01 end=2013-02-01 plot=1 plotbad=0 '\n",
    "          'plotpsd=0 plotpdf=1 verbose=1 ymax=5 xtype=frequency n=1 removeoutliers=1 method=4'.format(script))\n",
    "    print('\\n{} net=TA sta=M22K loc= chan=BHZ,BHN,BHE start=2017-01-01 end=2017-02-01 plot=1 plotbad=0 '\n",
    "          'plotpsd=0 plotpdf=1 verbose=1 ymax=6 xtype=frequency n=1 removeoutliers=0 method=4'.format(script))\n",
    "    print('\\n{} net=TA sta=E25K loc= chan=BHZ,BHN,BHE start=2017-01-01 end=2017-02-01 plot=1 plotbad=0 '\n",
    "          'plotpsd=0 plotpdf=1 verbose=1 ymax=5 xtype=frequency n=1 removeoutliers=0 method=4'.format(script))\n",
    "    print('\\n{} net=TA sta=E25K loc= chan=BHZ,BHN,BHE start=2017-07-01 end=2017-08-01 plot=1 plotbad=0 '\n",
    "          'plotpsd=0 plotpdf=1 verbose=1 ymax=5 xtype=frequency n=1 removeoutliers=0 method=4'.format(script))\n",
    "    print ('\\n\\nReferences:'\n",
    "           '\\nAlbarello, Dario & Lunedei, Enrico. (2013). Combining horizontal ambient vibration components for H/V \\n'\n",
    "           '\\tspectral ratio estimates. Geophysical Journal International. 194. 936-951. 10.1093/gji/ggt130.\\n'\n",
    "           '\\nFrancisco J Sanchez-Sesma, Francisco & Rodriguez, Miguel & Iturraran-Viveros, Ursula & Luzon, Francisco\\n'\n",
    "           '\\t& Campillo, Michel & Margerin, Ludovic & Garcia-Jerez, Antonio & Suarez, Martha & Santoyo, Miguel &\\n'\n",
    "           '\\nPeterson, J. (1993). Observations and modeling of seismic background noise, U.S. Geological Survey\\n'\n",
    "           '\\topen-file report (Vol. 93-322, p. 94). Albuquerque: U.S. Geological Survey.\\n'\n",
    "           '\\nRodriguez-Castellanos, A. (2011). A theory for microtremor H/V spectral ratio: Application for a\\n'\n",
    "           '\\tlayered medium. Geophysical Journal International. 186. 221-225. 10.1111/j.1365-246X.2011.05064.x.\\n'\n",
    "           '\\nGuidelines for the Implementation of the H/V Spectral Ratio Technique on Ambient Vibrations, December\\n'\n",
    "           '\\t2004  Project No. EVG1-CT-2000-00026 SESAME.\\n'\n",
    "           '\\t\\tftp://ftp.geo.uib.no/pub/seismo/SOFTWARE/SESAME/USER-GUIDELINES/SESAME-HV-User-Guidelines.pdf')\n",
    "\n",
    "    print ('\\n\\n\\n')\n",
    "\n",
    "\n",
    "def check_mark():\n",
    "    \"\"\"The default Windows terminal is not able to display the check mark character correctly.\n",
    "       This function returns another displayable character if platform is Windows\"\"\"\n",
    "    check = get_char(u'\\u2714')\n",
    "    if sys.platform == 'win32':\n",
    "        check = get_char(u'\\u039E')\n",
    "    return check\n",
    "\n",
    "\n",
    "def get_char(in_char):\n",
    "    \"\"\"Output character with proper encoding/decoding\"\"\"\n",
    "    if in_char in greek_chars.keys():\n",
    "        out_char = greek_chars[in_char].encode(encoding='utf-8')\n",
    "    else:\n",
    "        out_char = in_char.encode(encoding='utf-8')\n",
    "    return out_char.decode('utf-8')\n",
    "\n",
    "\n",
    "def time_it(_t):\n",
    "    \"\"\"Compute elapsed time since the last call.\"\"\"\n",
    "    t1 = time.time()\n",
    "    dt = t1 - _t\n",
    "    t = _t\n",
    "    if dt > 0.05:\n",
    "        print(f'[TIME] {dt:0.1f} s', flush=True)\n",
    "        t = t1\n",
    "    return t\n",
    "\n",
    "\n",
    "def date_range(_start, _end, _interval):\n",
    "    \"\"\"Break an interval to date ranges\n",
    "       this is used to avoid large requests that get rejected.\n",
    "    \"\"\"\n",
    "    if _interval <= 1:\n",
    "        _date_list = [_start, _end]\n",
    "    else:\n",
    "        _date_list = list()\n",
    "        from datetime import datetime\n",
    "        start_t = datetime.strptime(_start, '%Y-%m-%d')\n",
    "        end_t = datetime.strptime(_end, '%Y-%m-%d')\n",
    "        diff = (end_t - start_t) / _interval\n",
    "        if diff.days <= 1:\n",
    "            _date_list = [_start, _end]\n",
    "        else:\n",
    "            for _index in range(_interval):\n",
    "                _date_list.append((start_t + diff * _index).strftime('%Y-%m-%d'))\n",
    "            _date_list.append(end_t.strftime('%Y-%m-%d'))\n",
    "    return _date_list\n",
    "\n",
    "\n",
    "def print_peak_report(_station_header, _report_header, _peak, _reportinfo, _min_rank):\n",
    "    \"\"\"print a report of peak parameters\"\"\"\n",
    "    _index = list()\n",
    "    _rank = list()\n",
    "\n",
    "    if report_information:\n",
    "        report_file_name = os.path.join(param.reportDirectory, fileLib.baselineFileName(\n",
    "            network, station, location, channel))\n",
    "\n",
    "        # In mac(python 3) the following statement works perfectly with just open without encoding, but\n",
    "        # in windows(w10, python3) this is is not an option and we have to include the encoding='utf-8' param.\n",
    "        report_file = open(report_file_name, 'w', encoding='utf-8')\n",
    "\n",
    "        # Write the report to the report file.\n",
    "        report_file.write('\\n\\nPeaks:\\n'\n",
    "                          'Parameters and ranking (A0: peak amplitude, f0: peak frequency, {}: satisfied):\\n\\n'\n",
    "                          '\\t- amplitude clarity conditions:\\n'\n",
    "                          '\\t\\t. there exist one frequency f-, lying between f0/4 and f0, such that A0 / A(f-) > 2\\n'\n",
    "                          '\\t\\t. there exist one frequency f+, lying between f0 and 4*f0, such that A0 / A(f+) > 2\\n'\n",
    "                          '\\t\\t. A0 > 2\\n\\n'\n",
    "                          '\\t- amplitude stability conditions:\\n'\n",
    "                          '\\t\\t. peak appear within +/-5% on HVSR curves of mean +/- one standard deviation (f0+/f0-)\\n'\n",
    "                          '\\t\\t. {}f lower than a frequency dependent threshold {}(f)\\n'\n",
    "                          '\\t\\t. {}A lower than a frequency dependent threshold log {}(f)\\n'.\n",
    "                          format(check_mark(), get_char('sigma'), get_char('epsilon'), get_char('sigma'), \n",
    "                                 get_char('teta')))\n",
    "\n",
    "        # Also output the report to the terminal.\n",
    "        print('\\n\\nPeaks:\\n'\n",
    "              'Parameters and ranking (A0: peak amplitude, f0: peak frequency, {}: satisfied)):\\n\\n'\n",
    "              '\\t- amplitude clarity conditions:\\n'\n",
    "              '\\t\\t. there exist one frequency f-, lying between f0/4 and f0, such that A0 / A(f-) > 2\\n'\n",
    "              '\\t\\t. there exist one frequency f+, lying between f0 and 4*f0, such that A0 / A(f+) > 2\\n'\n",
    "              '\\t\\t. A0 > 2\\n\\n'\n",
    "              '\\t- amplitude stability conditions:\\n'\n",
    "              '\\t\\t. peak appear within +/-5% on HVSR curves of mean +/- one standard deviation (f0+/f0-)\\n'\n",
    "              '\\t\\t. {}f lower than a frequency dependent threshold {}(f)\\n'\n",
    "              '\\t\\t. {}A lower than a frequency dependent threshold log {}(f)\\n'.\n",
    "              format(check_mark(), get_char('sigma'), get_char('epsilon'), get_char('sigma'), get_char('teta')), \n",
    "              flush=True)\n",
    "\n",
    "    for _i, _peak_value in enumerate(_peak):\n",
    "        _index.append(_i)\n",
    "        _rank.append(_peak_value['Score'])\n",
    "    _list = list(zip(_rank, _index))\n",
    "    _list.sort(reverse=True)\n",
    "\n",
    "    if report_information:\n",
    "        report_file.write('\\n%47s %10s %22s %12s %12s %32s %32s %27s %22s %17s'\n",
    "                          % ('Net.Sta.Loc.Chan', '    f0    ', '        A0 > 2        ', '     f-      ', '    f+     ',\n",
    "                             '     f0- within ±5% of f0 &     ', '     f0+ within ±5% of f0       ',\n",
    "                             get_char('sigma') +\n",
    "                             'f < ' + get_char('epsilon') + ' * f0      ', get_char('sigma') + 'log HVSR < log' +\n",
    "                             get_char('teta') + '    ', '   Score/Max.    \\n'))\n",
    "        report_file.write('%47s %10s %22s %12s %12s %32s %32s %27s %22s %17s\\n'\n",
    "                          % (47 * separator_character, 10 * separator_character, 22 * separator_character,\n",
    "                             12 * separator_character, 12 * separator_character, 32 * separator_character,\n",
    "                             32 * separator_character, 27 * separator_character, 22 * separator_character,\n",
    "                             7 * separator_character))\n",
    "\n",
    "        print('\\n%47s %10s %22s %12s %12s %32s %32s %27s %22s %17s'\n",
    "                          % ('Net.Sta.Loc.Chan', '    f0    ', '        A0 > 2        ', '     f-      ', '    f+     ',\n",
    "                             '     f0- within ±5% of f0 &     ', '     f0+ within ±5% of f0       ',\n",
    "                             get_char('sigma') +\n",
    "                             'f < ' + get_char('epsilon') + ' * f0      ', get_char('sigma') + 'log HVSR < log' +\n",
    "                             get_char('teta') + '    ', '   Score/Max.    \\n'), flush=True)\n",
    "\n",
    "        print('%47s %10s %22s %12s %12s %32s %32s %27s %22s %17s\\n'\n",
    "              % (47 * separator_character, 10 * separator_character, 22 * separator_character,\n",
    "                 12 * separator_character, 12 * separator_character, 32 * separator_character,\n",
    "                 32 * separator_character, 27 * separator_character, 22 * separator_character,\n",
    "                 7 * separator_character), flush=True)\n",
    "\n",
    "    _peak_visible = list()\n",
    "    for _i, _list_value in enumerate(_list):\n",
    "        _index = _list_value[1]\n",
    "        _peak_found = _peak[_index]\n",
    "        if float(_peak_found['Score']) < _min_rank:\n",
    "            continue\n",
    "        else:\n",
    "            _peak_visible.append(True)\n",
    "\n",
    "        report_file.write('%47s %10.3f %22s %12s %12s %32s %32s %27s %22s %12d/%0d\\n' %\n",
    "              (_station_header, _peak_found['f0'], _peak_found['Report']['A0'], _peak_found['f-'], _peak_found['f+'],\n",
    "               _peak_found['Report']['P-'], _peak_found['Report']['P+'], _peak_found['Report']['Sf'],\n",
    "               _peak_found['Report']['Sa'], _peak_found['Score'], max_rank))\n",
    "\n",
    "        print('%47s %10.3f %22s %12s %12s %32s %32s %27s %22s %12d/%0d\\n' %\n",
    "              (_station_header, _peak_found['f0'], _peak_found['Report']['A0'], _peak_found['f-'], _peak_found['f+'],\n",
    "               _peak_found['Report']['P-'], _peak_found['Report']['P+'], _peak_found['Report']['Sf'],\n",
    "               _peak_found['Report']['Sa'], _peak_found['Score'], max_rank), flush=True)\n",
    "\n",
    "    if len(_list) <= 0 or len(_peak_visible) <= 0:\n",
    "        report_file.write('%47s\\n' % _station_header)\n",
    "        report_file.close()\n",
    "\n",
    "        print('%47s\\n' % _station_header, flush=True)\n",
    "\n",
    "\n",
    "def get_args(_arg_list):\n",
    "    \"\"\"get the run arguments\"\"\"\n",
    "    _args = {}\n",
    "    for _i in range(1, len(_arg_list)):\n",
    "        try:\n",
    "            _key, _value = _arg_list[_i].split('=')\n",
    "            _args[_key] = _value\n",
    "        except Exception as _e:\n",
    "            msgLib.error('Bad parameter: {}, will use the default\\n{}'.format(_arg_list[_i], _e), 1)\n",
    "            continue\n",
    "    return _args\n",
    "\n",
    "\n",
    "def get_param(_args, _key, _msg_lib, _value, be_verbose=-1):\n",
    "    \"\"\"get a run argument for the given _key\"\"\"\n",
    "    if _key in _args.keys():\n",
    "        if be_verbose >= 0:\n",
    "            print (_key, _args[_key])\n",
    "        return _args[_key]\n",
    "    elif _value is not None:\n",
    "        return _value\n",
    "    else:\n",
    "        _msg_lib.error('missing parameter {}'.format(_key), 1)\n",
    "        usage()\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "def check_y_range(_y, _low, _high):\n",
    "    \"\"\"check the PSD values to see if they are within the range\"\"\"\n",
    "    _ok = list()\n",
    "    _not_ok = list()\n",
    "\n",
    "    # use subtract operator to see if y and _low/_high are crossing\n",
    "    for _i, _value in enumerate(_y):\n",
    "        _l = [_a - _b for _a, _b in zip(_value, _low)]\n",
    "        if min(_l) < 0:\n",
    "            _not_ok.append(_i)\n",
    "            continue\n",
    "\n",
    "        _h = [_a - _b for _a, _b in zip(_value, _high)]\n",
    "        if max(_h) > 0:\n",
    "            _not_ok.append(_i)\n",
    "            continue\n",
    "\n",
    "        _ok.append(_i)\n",
    "\n",
    "    return _ok, _not_ok\n",
    "\n",
    "\n",
    "def remove_db(_db_value):\n",
    "    \"\"\"convert dB power to power\"\"\"\n",
    "    _values = list()\n",
    "    for _d in _db_value:\n",
    "        _values.append(10 ** (float(_d) / 10.0))\n",
    "    return _values\n",
    "\n",
    "\n",
    "def get_power(_db, _x):\n",
    "    \"\"\"calculate HVSR\n",
    "      We will undo setp 6 of MUSTANG processing as outlined below:\n",
    "          1. Dividing the window into 13 segments having 75% overlap\n",
    "          2. For each segment:\n",
    "             2.1 Removing the trend and mean\n",
    "             2.2 Apply a 10% sine taper\n",
    "             2.3 FFT\n",
    "          3. Calculate the normalized PSD\n",
    "          4. Average the 13 PSDs & scale to compensate for tapering\n",
    "          5. Frequency-smooth the averaged PSD over 1-octave intervals at 1/8-octave increments\n",
    "          6. Convert power to decibels\n",
    "\n",
    "    NOTE: PSD is equal to the power divided by the width of the bin\n",
    "          PSD = P / W\n",
    "          log(PSD) = Log(P) - log(W)\n",
    "          log(P) = log(PSD) + log(W)  here W is width in frequency\n",
    "          log(P) = log(PSD) - log(Wt) here Wt is width in period\n",
    "\n",
    "    for each bin perform rectangular integration to compute power\n",
    "    power is assigned to the point at the begining of the interval\n",
    "         _   _\n",
    "        | |_| |\n",
    "        |_|_|_|\n",
    "\n",
    "     Here we are computing power for individual ponts, so, no integration is necessary, just\n",
    "     compute area\n",
    "    \"\"\"\n",
    "    _dx = np.diff(_x)[0]\n",
    "    _p = np.multiply(np.mean(remove_db(_db)), _dx)\n",
    "    return _p\n",
    "\n",
    "\n",
    "def get_hvsr(_dbz, _db1, _db2, _x, use_method=4):\n",
    "    \"\"\"\n",
    "    H is computed based on the selected use_method see: https://academic.oup.com/gji/article/194/2/936/597415\n",
    "        use_method:\n",
    "           (1) DFA\n",
    "           (2) arithmetic mean, that is, H ≡ (HN + HE)/2\n",
    "           (3) geometric mean, that is, H ≡ √HN · HE, recommended by the SESAME project (2004)\n",
    "           (4) vector summation, that is, H ≡ √H2 N + H2 E\n",
    "           (5) quadratic mean, that is, H ≡ √(H2 N + H2 E )/2\n",
    "           (6) maximum horizontal value, that is, H ≡ max {HN, HE}\n",
    "    \"\"\"\n",
    "    _pz = get_power(_dbz, _x)\n",
    "    _p1 = get_power(_db1, _x)\n",
    "    _p2 = get_power(_db2, _x)\n",
    "\n",
    "    _hz = math.sqrt(_pz)\n",
    "    _h1 = math.sqrt(_p1)\n",
    "    _h2 = math.sqrt(_p2)\n",
    "\n",
    "    _h = {2: (_h1 + _h2) / 2.0, 3: math.sqrt(_h1 * _h2), 4: math.sqrt(_p1 + _p2), 5: math.sqrt((_p1 + _p2) / 2.0),\n",
    "          6: max(_h1, _h2)}\n",
    "\n",
    "    _hvsr = _h[use_method] / _hz\n",
    "    return _hvsr\n",
    "\n",
    "\n",
    "def find_peaks(_y):\n",
    "    \"\"\"find peaks\"\"\"\n",
    "    _index_list = argrelextrema(np.array(_y), np.greater)\n",
    "\n",
    "    return _index_list[0]\n",
    "\n",
    "\n",
    "def init_peaks(_x, _y, _index_list, _hvsr_band, _peak_water_level):\n",
    "    \"\"\"initialize peaks\"\"\"\n",
    "    _peak = list()\n",
    "    for _i in indexList:\n",
    "        if _y[_i] > _peak_water_level[_i] and (_hvsr_band[0] <= _x[_i] <= _hvsr_band[1]):\n",
    "            _peak.append({'f0': float(_x[_i]), 'A0': float(_y[_i]), 'f-': None, 'f+': None, 'Sf': None, 'Sa': None,\n",
    "                          'Score': 0, 'Report': {'A0': '', 'Sf': '', 'Sa': '', 'P+': '', 'P-': ''}})\n",
    "    return _peak\n",
    "\n",
    "\n",
    "def check_clarity(_x, _y, _peak, do_rank=False):\n",
    "    \"\"\"\n",
    "       test peaks for satisfying amplitude clarity conditions as outlined by SESAME 2004:\n",
    "           - there exist one frequency f-, lying between f0/4 and f0, such that A0 / A(f-) > 2\n",
    "           - there exist one frequency f+, lying between f0 and 4*f0, such that A0 / A(f+) > 2\n",
    "           - A0 > 2\n",
    "    \"\"\"\n",
    "    global max_rank\n",
    "\n",
    "    # Peaks with A0 > 2.\n",
    "    if do_rank:\n",
    "        max_rank += 1\n",
    "    _a0 = 2.0\n",
    "    for _i in range(len(_peak)):\n",
    "\n",
    "        if float(_peak[_i]['A0']) > _a0:\n",
    "            _peak[_i]['Report']['A0'] = '%10.2f > %0.1f %1s' % (_peak[_i]['A0'], _a0, check_mark())\n",
    "            _peak[_i]['Score'] += 1\n",
    "        else:\n",
    "            _peak[_i]['Report']['A0'] = '%10.2f > %0.1f  ' % (_peak[_i]['A0'], _a0)\n",
    "\n",
    "    # Test each _peak for clarity.\n",
    "    if do_rank:\n",
    "        max_rank += 1\n",
    "    for _i in range(len(_peak)):\n",
    "        _peak[_i]['f-'] = '-'\n",
    "        for _j in range(len(_x) - 1, -1, -1):\n",
    "\n",
    "            # There exist one frequency f-, lying between f0/4 and f0, such that A0 / A(f-) > 2.\n",
    "            if (float(_peak[_i]['f0']) / 4.0 <= _x[_j] < float(_peak[_i]['f0'])) and \\\n",
    "                    float(_peak[_i]['A0']) / _y[_j] > 2.0:\n",
    "                _peak[_i]['f-'] = '%10.3f %1s' % (_x[_j], check_mark())\n",
    "                _peak[_i]['Score'] += 1\n",
    "                break\n",
    "\n",
    "    if do_rank:\n",
    "        max_rank += 1\n",
    "    for _i in range(len(_peak)):\n",
    "        _peak[_i]['f+'] = '-'\n",
    "        for _j in range(len(_x) - 1):\n",
    "\n",
    "            # There exist one frequency f+, lying between f0 and 4*f0, such that A0 / A(f+) > 2.\n",
    "            if float(_peak[_i]['f0']) * 4.0 >= _x[_j] > float(_peak[_i]['f0']) and \\\n",
    "                    float(_peak[_i]['A0']) / _y[_j] > 2.0:\n",
    "                _peak[_i]['f+'] = '%10.3f %1s' % (_x[_j], check_mark())\n",
    "                _peak[_i]['Score'] += 1\n",
    "                break\n",
    "\n",
    "    return _peak\n",
    "\n",
    "\n",
    "def check_freq_stability(_peak, _peakm, _peakp):\n",
    "    \"\"\"\n",
    "       test peaks for satisfying stability conditions as outlined by SESAME 2004:\n",
    "           - the _peak should appear at the same frequency (within a percentage ± 5%) on the H/V\n",
    "             curves corresponding to mean + and – one standard deviation.\n",
    "    \"\"\"\n",
    "    global max_rank\n",
    "\n",
    "    #\n",
    "    # check σf and σA\n",
    "    #\n",
    "    max_rank += 1\n",
    "\n",
    "    _found_m = list()\n",
    "    for _i in range(len(_peak)):\n",
    "        _dx = 1000000.\n",
    "        _found_m.append(False)\n",
    "        _peak[_i]['Report']['P-'] = '- &'\n",
    "        for _j in range(len(_peakm)):\n",
    "            if abs(_peakm[_j]['f0'] - _peak[_i]['f0']) < _dx:\n",
    "                _index = _j\n",
    "                _dx = abs(_peakm[_j]['f0'] - _peak[_i]['f0'])\n",
    "            if peak[_i]['f0'] * 0.95 <= _peakm[_j]['f0'] <= _peak[_i]['f0'] * 1.05:\n",
    "                _peak[_i]['Report']['P-'] = '%0.3f within ±5%s of %0.3f %1s' % (_peakm[_j]['f0'], '%',\n",
    "                                                                                 _peak[_i]['f0'], '&')\n",
    "                _found_m[_i] = True\n",
    "                break\n",
    "        if _peak[_i]['Report']['P-'] == '-':\n",
    "            _peak[_i]['Report']['P-'] = '%0.3f within ±5%s of %0.3f %1s' % (_peakm[_i]['f0'], '%',\n",
    "                                                                             _peak[_i]['f0'], '&')\n",
    "\n",
    "    _found_p = list()\n",
    "    for _i in range(len(_peak)):\n",
    "        _dx = 1000000.\n",
    "        _found_p.append(False)\n",
    "        _peak[_i]['Report']['P+'] = '-'\n",
    "        for _j in range(len(_peakp)):\n",
    "            if abs(_peakp[_j]['f0'] - _peak[_i]['f0']) < _dx:\n",
    "                _index = _j\n",
    "                _dx = abs(_peakp[_j]['f0'] - _peak[_i]['f0'])\n",
    "            if _peak[_i]['f0'] * 0.95 <= _peakp[_j]['f0'] <= _peak[_i]['f0'] * 1.05:\n",
    "                if _found_m[_i]:\n",
    "                    _peak[_i]['Report']['P+'] = '%0.3f within ±5%s of %0.3f %1s' % (\n",
    "                        _peakp[_j]['f0'], '%', _peak[_i]['f0'], check_mark())\n",
    "                    _peak[_i]['Score'] += 1\n",
    "                else:\n",
    "                    _peak[_i]['Report']['P+'] = '%0.3f within ±5%s of %0.3f %1s' % (\n",
    "                        _peakp[_i]['f0'], '%', _peak[_i]['f0'], ' ')\n",
    "                break\n",
    "        if _peak[_i]['Report']['P+'] == '-' and len(_peakp) > 0:\n",
    "            _peak[_i]['Report']['P+'] = '%0.3f within ±5%s of %0.3f %1s' % (\n",
    "                _peakp[_i]['f0'], '%', _peak[_i]['f0'], ' ')\n",
    "\n",
    "    return _peak\n",
    "\n",
    "\n",
    "def check_stability(_stdf, _peak, _hvsr_log_std, rank):\n",
    "    \"\"\"\n",
    "    test peaks for satisfying stability conditions as outlined by SESAME 2004:\n",
    "       - σf lower than a frequency dependent threshold ε(f)\n",
    "       - σA (f0) lower than a frequency dependent threshold θ(f),\n",
    "    \"\"\"\n",
    "\n",
    "    global max_rank\n",
    "\n",
    "    #\n",
    "    # check σf and σA\n",
    "    #\n",
    "    if rank:\n",
    "        max_rank += 2\n",
    "    for _i in range(len(_peak)):\n",
    "        _peak[_i]['Sf'] = _stdf[_i]\n",
    "        _peak[_i]['Sa'] = _hvsr_log_std[_i]\n",
    "        _this_peak = _peak[_i]\n",
    "        if _this_peak['f0'] < 0.2:\n",
    "            _e = 0.25\n",
    "            if _stdf[_i] < _e * _this_peak['f0']:\n",
    "                _peak[_i]['Report']['Sf'] = '%10.4f < %0.2f * %0.3f %1s' % (_stdf[_i], _e, _this_peak['f0'],\n",
    "                                                                            check_mark())\n",
    "                _this_peak['Score'] += 1\n",
    "            else:\n",
    "                _peak[_i]['Report']['Sf'] = '%10.4f < %0.2f * %0.3f  ' % (_stdf[_i], _e, _this_peak['f0'])\n",
    "\n",
    "            _t = 0.48\n",
    "            if _hvsr_log_std[_i] < _t:\n",
    "                _peak[_i]['Report']['Sa'] = '%10.4f < %0.2f %1s' % (_hvsr_log_std[_i], _t,\n",
    "                                                                    check_mark())\n",
    "                _this_peak['Score'] += 1\n",
    "            else:\n",
    "                _peak[_i]['Report']['Sa'] = '%10.4f < %0.2f  ' % (_hvsr_log_std[_i], _t)\n",
    "\n",
    "        elif 0.2 <= _this_peak['f0'] < 0.5:\n",
    "            _e = 0.2\n",
    "            if _stdf[_i] < _e * _this_peak['f0']:\n",
    "                _peak[_i]['Report']['Sf'] = '%10.4f < %0.2f * %0.3f %1s' % (_stdf[_i], _e, _this_peak['f0'],\n",
    "                                                                            check_mark())\n",
    "                _this_peak['Score'] += 1\n",
    "            else:\n",
    "                _peak[_i]['Report']['Sf'] = '%10.4f < %0.2f * %0.3f  ' % (_stdf[_i], _e, _this_peak['f0'])\n",
    "\n",
    "            _t = 0.40\n",
    "            if _hvsr_log_std[_i] < _t:\n",
    "                _peak[_i]['Report']['Sa'] = '%10.4f < %0.2f %1s' % (_hvsr_log_std[_i], _t,\n",
    "                                                                    check_mark())\n",
    "                _this_peak['Score'] += 1\n",
    "            else:\n",
    "                _peak[_i]['Report']['Sa'] = '%10.4f < %0.2f  ' % (_hvsr_log_std[_i], _t)\n",
    "\n",
    "        elif 0.5 <= _this_peak['f0'] < 1.0:\n",
    "            _e = 0.15\n",
    "            if _stdf[_i] < _e * _this_peak['f0']:\n",
    "                _peak[_i]['Report']['Sf'] = '%10.4f < %0.2f * %0.3f %1s' % (_stdf[_i], _e, _this_peak['f0'],\n",
    "                                                                            check_mark())\n",
    "                _this_peak['Score'] += 1\n",
    "            else:\n",
    "                _peak[_i]['Report']['Sf'] = '%10.4f < %0.2f * %0.3f  ' % (_stdf[_i], _e, _this_peak['f0'])\n",
    "\n",
    "            _t = 0.3\n",
    "            if _hvsr_log_std[_i] < _t:\n",
    "                _peak[_i]['Report']['Sa'] = '%10.4f < %0.2f %1s' % (_hvsr_log_std[_i], _t, check_mark())\n",
    "                _this_peak['Score'] += 1\n",
    "            else:\n",
    "                _peak[_i]['Report']['Sa'] = '%10.4f < %0.2f  ' % (_hvsr_log_std[_i], _t)\n",
    "\n",
    "        elif 1.0 <= _this_peak['f0'] <= 2.0:\n",
    "            _e = 0.1\n",
    "            if _stdf[_i] < _e * _this_peak['f0']:\n",
    "                _peak[_i]['Report']['Sf'] = '%10.4f < %0.2f * %0.3f %1s' % (_stdf[_i], _e, _this_peak['f0'],\n",
    "                                                                            check_mark())\n",
    "                _this_peak['Score'] += 1\n",
    "            else:\n",
    "                _peak[_i]['Report']['Sf'] = '%10.4f < %0.2f * %0.3f  ' % (_stdf[_i], _e, _this_peak['f0'])\n",
    "\n",
    "            _t = 0.25\n",
    "            if _hvsr_log_std[_i] < _t:\n",
    "                _peak[_i]['Report']['Sa'] = '%10.4f < %0.2f %1s' % (_hvsr_log_std[_i], _t, check_mark())\n",
    "                _this_peak['Score'] += 1\n",
    "            else:\n",
    "                _peak[_i]['Report']['Sa'] = '%10.4f < %0.2f  ' % (_hvsr_log_std[_i], _t)\n",
    "\n",
    "        elif _this_peak['f0'] > 0.2:\n",
    "            _e = 0.05\n",
    "            if _stdf[_i] < _e * _this_peak['f0']:\n",
    "                _peak[_i]['Report']['Sf'] = '%10.4f < %0.2f * %0.3f %1s' % (_stdf[_i], _e, _this_peak['f0'],\n",
    "                                                                            check_mark())\n",
    "                _this_peak['Score'] += 1\n",
    "            else:\n",
    "                _peak[_i]['Report']['Sf'] = '%10.4f < %0.2f * %0.3f  ' % (_stdf[_i], _e, _this_peak['f0'])\n",
    "\n",
    "            _t = 0.2\n",
    "            if _hvsr_log_std[_i] < _t:\n",
    "                _peak[_i]['Report']['Sa'] = '%10.4f < %0.2f %1s' % (_hvsr_log_std[_i], _t, check_mark())\n",
    "                _this_peak['Score'] += 1\n",
    "            else:\n",
    "                _peak[_i]['Report']['Sa'] = '%10.4f < %0.2f  ' % (_hvsr_log_std[_i], _t)\n",
    "    return _peak\n",
    "\n",
    "\n",
    "def get_pdf(_url, _verbose):\n",
    "    \"\"\"get PDF\"\"\"\n",
    "    _x_values = list()\n",
    "    _y_values = list()\n",
    "    _x = list()\n",
    "    _y = list()\n",
    "    _p = list()\n",
    "\n",
    "    if _verbose >= 0:\n",
    "        msgLib.info('requesting:' + _url)\n",
    "    try:\n",
    "        _link = urllib.request.urlopen(_url)\n",
    "    except Exception as _e:\n",
    "        msgLib.error('\\n\\nReceived HTTP Error code: {}\\n{}'.format(_e.code, _e.reason), 1)\n",
    "        if _e.code == 404:\n",
    "            _url_items = _url.split('&')\n",
    "            _starttime = [x for x in _url_items if x.startswith('starttime')]\n",
    "            _endtime = [x for x in _url_items if x.startswith('endtime')]\n",
    "            msgLib.error('Error 404: PDF not found in the range {} and {} when requested:\\n{}'.format(\n",
    "                _starttime.split('=')[1], _endtime.split('=')[1], _url), 1)\n",
    "        elif _e.code == 413:\n",
    "            print('Note: Either use the run argument \"n\" to split the requested date range to smaller intervals'\n",
    "                  '\\nCurrent \"n\"\" value is: {}. Or request a shorter time interval.'.format(n), flush=True)\n",
    "            sys.exit(1)\n",
    "        msgLib.error('failed on target {} {}'.format(target, URL), 1)\n",
    "        return _x, _y, _p\n",
    "\n",
    "    if _verbose >= 0:\n",
    "        msgLib.info('PDF waiting for reply....')\n",
    "\n",
    "    _data = _link.read().decode()\n",
    "    _link.close()\n",
    "    _lines = _data.split('\\n')\n",
    "    _last_frequency = ''\n",
    "    _line_count = 0\n",
    "    _non_blank_last_line = 0\n",
    "    _hits_list = list()\n",
    "    _power_list = list()\n",
    "    if len(_lines[-1].strip()) <= 0:\n",
    "        _non_blank_last_line = 1\n",
    "    for _line in _lines:\n",
    "        _line_count += 1\n",
    "        if len(_line.strip()) <= 0:\n",
    "            continue\n",
    "        if _line[0] == '#' or ',' not in _line:\n",
    "            continue\n",
    "        (_freq, _power, _hits) = _line.split(',')\n",
    "        if _last_frequency == '':\n",
    "            _last_frequency = _freq.strip()\n",
    "            _power_list = list()\n",
    "            _power_list.append(float(_power))\n",
    "            _hits_list = list()\n",
    "            _hits_list.append(int(_hits))\n",
    "        elif _last_frequency == _freq.strip():\n",
    "            _power_list.append(float(_power))\n",
    "            _hits_list.append(int(_hits))\n",
    "        if _last_frequency != _freq.strip() or _line_count == len(_lines) - _non_blank_last_line:\n",
    "            _total_hits = sum(_hits_list)\n",
    "            _y_values.append(np.array(_hits_list) * 100.0 / _total_hits)\n",
    "            if xtype == 'period':\n",
    "                last_x = 1.0 / float(_last_frequency)\n",
    "                _x_values.append(last_x)\n",
    "            else:\n",
    "                last_x = float(_last_frequency)\n",
    "                _x_values.append(last_x)\n",
    "            for _i in range(len(_hits_list)):\n",
    "                _y.append(float(_power_list[_i]))\n",
    "                _p.append(float(_hits_list[_i]) * 100.0 / float(_total_hits))\n",
    "                _x.append(last_x)\n",
    "\n",
    "            _last_frequency = _freq.strip()\n",
    "            _power_list = list()\n",
    "            _power_list.append(float(_power))\n",
    "            _hits_list = list()\n",
    "            _hits_list.append(int(_hits))\n",
    "    return _x, _y, _p\n",
    "\n",
    "\n",
    "# Set run parameters.\n",
    "args = get_args(sys.argv)\n",
    "if len(args) <= 1:\n",
    "    usage()\n",
    "    sys.exit()\n",
    "\n",
    "verbose = int(get_param(args, 'verbose', msgLib, -1, be_verbose=param.verbose))\n",
    "if verbose >= 0:\n",
    "    print('\\n[INFO]', sys.argv[0], version)\n",
    "\n",
    "report_information = int(get_param(args, 'report_information', msgLib, 1, be_verbose=verbose))\n",
    "\n",
    "# Get channels and sort them in the reverse ] order to make sure that we always have ?HZ first.\n",
    "# Order of the horizontals is not important.\n",
    "channels = get_param(args, 'chan', msgLib, param.chan)\n",
    "channelList = sorted(channels.split(','), reverse=True)\n",
    "if len(channelList) < 3:\n",
    "    msgLib.error('need 3 channels!', 1)\n",
    "    sys.exit()\n",
    "\n",
    "sorted_channel_list = channelList.copy()\n",
    "for channel in channelList:\n",
    "    sorted_channel_list[channel_order[channel[2]]] = channel\n",
    "\n",
    "# See if we want to reject suspect PSDs.\n",
    "remove_outliers = bool(int(get_param(args, 'removeoutliers', msgLib, False)))\n",
    "msgLib.info('remove_outliers: {}'.format(remove_outliers))\n",
    "\n",
    "# Minimum SESAME 2004 rank to be accepted.\n",
    "min_rank = float(get_param(args, 'minrank', msgLib, param.minrank))\n",
    "\n",
    "# network, station, and location to process.\n",
    "network = get_param(args, 'net', msgLib, None)\n",
    "if network is None:\n",
    "    msgLib.error('network not defined!', 1)\n",
    "    sys.exit()\n",
    "station = get_param(args, 'sta', msgLib, None)\n",
    "if station is None:\n",
    "    msgLib.error('station not defined!', 1)\n",
    "    sys.exit()\n",
    "location = get_param(args, 'loc', msgLib, '*')\n",
    "if location is None:\n",
    "    msgLib.error('location not defined!', 1)\n",
    "    sys.exit()\n",
    "\n",
    "# Start of the window.\n",
    "start = get_param(args, 'start', msgLib, None)\n",
    "start_hour = 'T00:00:00'\n",
    "start_time = time.strptime(start, '%Y-%m-%d')\n",
    "end = get_param(args, 'end', msgLib, None)\n",
    "\n",
    "# If start and end are the same day, process that day.\n",
    "if start.strip() == end.strip():\n",
    "    end_hour = 'T23:59:59'\n",
    "else:\n",
    "    end_hour = 'T00:00:00'\n",
    "end_time = time.strptime(end, '%Y-%m-%d')\n",
    "\n",
    "# Break the start-end interval to n segments.\n",
    "n = int(get_param(args, 'n', msgLib, 1))\n",
    "date_list = date_range(start, end, n)\n",
    "msgLib.info('DATE LIST: {}'.format(date_list))\n",
    "\n",
    "# How to combine h1 & h2.\n",
    "method = int(get_param(args, 'method', msgLib, param.method))\n",
    "if method <= 0 or method > 6:\n",
    "    msgLib.error('method {} for combining H1 & H2 is invalid!'.format(method), 1)\n",
    "    sys.exit()\n",
    "elif method == 1:\n",
    "    dfa = 1\n",
    "else:\n",
    "    dfa = 0\n",
    "\n",
    "msgLib.info('Combining H1 and H2 Using {} method'.format(param.methodList[method]))\n",
    "\n",
    "do_plot = int(get_param(args, 'plot', msgLib, param.plot))\n",
    "show_plot = int(get_param(args, 'showplot', msgLib, param.plot))\n",
    "plot_psd = int(get_param(args, 'plotpsd', msgLib, param.plotpsd))\n",
    "plot_pdf = int(get_param(args, 'plotpdf', msgLib, param.plotpdf))\n",
    "plot_bad = int(get_param(args, 'plotbad', msgLib, param.plotbad))\n",
    "plot_nnm = int(get_param(args, 'plotnnm', msgLib, param.plotnnm))\n",
    "\n",
    "day_values_passed = [[], [], []]\n",
    "water_level = float(get_param(args, 'waterlevel', msgLib, param.waterlevel))\n",
    "hvsr_ylim = param.hvsrylim\n",
    "hvsr_ylim[1] = float(get_param(args, 'ymax', msgLib, param.hvsrylim[1]))\n",
    "xtype = get_param(args, 'xtype', msgLib, param.xtype)\n",
    "hvsr_band = get_param(args, 'hvsrband', msgLib, param.hvsrband)\n",
    "\n",
    "report_header = '.'.join([network, station, location, '-'.join(sorted_channel_list)])\n",
    "station_header = report_header\n",
    "station_header = '{} {} {}'.format(station_header, start, end)\n",
    "report_header += ' {} from {} to {}\\nusing {}'.format(report_header, start, end, param.methodList[method])\n",
    "plot_title = report_header\n",
    "report_header = '{}\\n\\n'.format(report_header)\n",
    "\n",
    "# Turn off the display requirement if not needed.\n",
    "if not show_plot:\n",
    "    if verbose >= 0:\n",
    "        msgLib.info('Plot Off')\n",
    "    matplotlib.use('agg')\n",
    "else:\n",
    "    from obspy.imaging.cm import pqlx\n",
    "    from obspy.signal.spectral_estimation import get_nlnm, get_nhnm\n",
    "\n",
    "ax2 = None\n",
    "# Do one channel at a time.\n",
    "channel_index = -1\n",
    "for channel in sorted_channel_list:\n",
    "    channel_index += 1\n",
    "    x_values = list()\n",
    "    psd_values = list()\n",
    "    day_values = list()\n",
    "    day_time_values = list()\n",
    "    pct_low = list()\n",
    "    pct_high = list()\n",
    "    pct_mid = list()\n",
    "\n",
    "    target = '.'.join([network, station, location, channel, '*'])\n",
    "    label = '.'.join([network, station, location, 'PSDs'])\n",
    "    label_hvsr = '.'.join([network, station, location, 'HVSR'])\n",
    "    if verbose >= 0:\n",
    "        msgLib.info('requesting {} from {} to {}'.format(target, start, end))\n",
    "\n",
    "    # Baseline files are required if we will remove the outliers. We assume the baseline file has all the periods,\n",
    "    # so we use it as a reference.\n",
    "    if remove_outliers:\n",
    "        try:\n",
    "            baselineFile = open(\n",
    "                os.path.join(param.baselineDirectory, fileLib.baselineFileName(network, station, location, channel)),\n",
    "                'r')\n",
    "        except Exception as e:\n",
    "            msgLib.error('Failed to read baseline file {}\\n'\n",
    "                         'Use the getStationChannelBaseline.py script to generate the baseline file or '\n",
    "                         'set the parameter removeoutliers=0.'.\n",
    "                         format(os.path.join(param.baselineDirectory, fileLib.baselineFileName(network,\n",
    "                                                                                               station,\n",
    "                                                                                               location,\n",
    "                                                                                               channel))), 1)\n",
    "            sys.exit()\n",
    "\n",
    "        lines = baselineFile.read()\n",
    "        baseline = lines.split('\\n')\n",
    "        for index_value in range(0, len(baseline)):\n",
    "            if len(baseline[index_value].strip()) == 0:\n",
    "                continue\n",
    "            if baseline[index_value].strip().startswith('#'):\n",
    "                values = baseline[index_value].strip().split()\n",
    "                percent_low = values[1]\n",
    "                percent_mid = values[3]\n",
    "                percent_high = values[5]\n",
    "                continue\n",
    "\n",
    "            values = baseline[index_value].split()\n",
    "\n",
    "            x_values.append(float(values[0]))\n",
    "            pct_low.append(float(values[1]))\n",
    "            pct_mid.append(float(values[2]))\n",
    "            pct_high.append(float(values[3]))\n",
    "        baselineFile.close()\n",
    "\n",
    "    # Get daily PSDs from MUSTANG.\n",
    "    # Limit PSD segments starting between starttime (inclusive) and endtime (exclusive)\n",
    "    pdf_x = list()\n",
    "    pdf_y = list()\n",
    "    pdfP = list()\n",
    "    for date_index in range(len(date_list) - 1):\n",
    "        msgLib.info('Doing {}{} to {}{}'.format(date_list[date_index], start_hour, date_list[date_index + 1], end_hour))\n",
    "        URL = '{}target={}&starttime={}{}&endtime={}{}&format=xml&correct=true'.format(param.mustangPsdUrl, target,\n",
    "                                                                                       date_list[date_index],\n",
    "                                                                                       start_hour,\n",
    "                                                                                       date_list[date_index + 1],\n",
    "                                                                                       end_hour)\n",
    "        if verbose >= 0:\n",
    "            msgLib.info('requesting: {}'.format(URL))\n",
    "            t0 = time_it(t0)\n",
    "        try:\n",
    "            link = urllib.request.urlopen(URL)\n",
    "        except Exception as _e:\n",
    "            msgLib.error('\\n\\nReceived HTTP Error code: {}\\n{}'.format(_e.code, _e.reason), 1)\n",
    "            if _e.code == 404:\n",
    "                msgLib.error('Error 404: No PSDs found in the range {}{} to {}{} when requested:\\n\\n{}'.format(\n",
    "                    date_list[date_index], start_hour, date_list[date_index + 1], end_hour, URL), 1)\n",
    "                continue\n",
    "            elif _e.code == 413:\n",
    "                print('Note: Either use the run argument \"n\" to split the requested date range to smaller intervals'\n",
    "                      '\\nCurrent \"n\"\" value is: {}. Or request a shorter time interval.'.format(n), flush=True)\n",
    "                sys.exit(1)\n",
    "            msgLib.error('failed on target {} {}'.format(target, URL), 1)\n",
    "\n",
    "        if verbose:\n",
    "            msgLib.info('PSD waiting for reply....')\n",
    "\n",
    "        tree = ET.parse(link)\n",
    "        link.close()\n",
    "        root = tree.getroot()\n",
    "\n",
    "        if verbose:\n",
    "            requestStart = root.find('RequestedDateRange').find('Start').text\n",
    "            requestEnd = root.find('RequestedDateRange').find('End').text\n",
    "\n",
    "        psds = root.find('Psds')\n",
    "\n",
    "        all_psds = psds.findall('Psd')\n",
    "        if verbose:\n",
    "            msgLib.info('PSD: {}'.format(str(len(all_psds))))\n",
    "            t0 = time_it(t0)\n",
    "\n",
    "        for psd in all_psds:\n",
    "            day = psd.attrib['start'].split('T')[0]\n",
    "            psdTime = time.strptime(day, '%Y-%m-%d')\n",
    "            if (start_time != end_time and (psdTime < start_time or psdTime >= end_time)) or \\\n",
    "                    (start_time == end_time and psdTime != start_time):\n",
    "                if verbose >= 0:\n",
    "                    msgLib.warning(sys.argv[0], 'Rejected, PSD of {} is outside the  window {} to {}'.\n",
    "                                   format(psd.attrib['start'],\n",
    "                                          time.strftime('%Y-%m-%dT%H:%M:%S', start_time),\n",
    "                                          time.strftime('%Y-%m-%dT%H:%M:%S', end_time)))\n",
    "                continue\n",
    "            allValues = psd.findall('value')\n",
    "\n",
    "            X = list()\n",
    "            Y = list()\n",
    "            for value in allValues:\n",
    "                X.append(float(value.attrib['freq']))\n",
    "                Y.append(float(value.attrib['power']))\n",
    "\n",
    "            # We follow a simple logic, the X values must match. We take the first one to be the sequence we want.\n",
    "            if not x_values:\n",
    "                x_values = list(X)\n",
    "\n",
    "            if X != x_values:\n",
    "                if verbose:\n",
    "                    msgLib.warning(sys.argv[0], 'Rejected {} {} {} for bad X'.format(target, date_list[date_index],\n",
    "                                                                                     date_list[date_index + 1]))\n",
    "            else:\n",
    "                # Store the PSD values and at the same time keep track of their day and time.\n",
    "                day_values.append(day)\n",
    "                day_time_values.append(psd.attrib['start'])\n",
    "                psd_values.append(Y)\n",
    "\n",
    "        if plot_pdf:\n",
    "            (thisX, thisY, thisP) = get_pdf('{}target={}&starttime={}{}&endtime={}{}&format=text'.format(\n",
    "                param.mustangPdfUrl, target, date_list[date_index], start_hour, date_list[date_index + 1],\n",
    "                end_hour), verbose)\n",
    "            pdf_x += thisX\n",
    "            pdf_y += thisY\n",
    "            pdfP += thisP\n",
    "            if verbose:\n",
    "                msgLib.info('PDF: {}'.format(len(pdf_y)))\n",
    "\n",
    "    # Must have PSDs.\n",
    "    if not psd_values:\n",
    "        msgLib.error('no PSDs found to process between {} and {}'.format(\n",
    "            date_list[date_index], date_list[date_index + 1]), 1)\n",
    "        sys.exit()\n",
    "    else:\n",
    "        if verbose >= 0:\n",
    "            msgLib.info('total PSDs:' + str(len(psd_values)))\n",
    "            t0 = time_it(t0)\n",
    "\n",
    "    # PSDs:\n",
    "    # Initial settings.\n",
    "    if channel_index == 0:\n",
    "        if do_plot:\n",
    "            if verbose >= 0:\n",
    "                msgLib.info('PLOT PSD')\n",
    "\n",
    "            fig = plt.figure(figsize=param.imageSize, facecolor='white')\n",
    "            ax = list()\n",
    "            fig.canvas.set_window_title(label)\n",
    "            ax.append(plt.subplot(plotRows, 1, channel_index + 1))\n",
    "\n",
    "        # [chanZ[day],chan1[day],chan2[day]]\n",
    "        daily_psd = [{}, {}, {}]\n",
    "        day_time_psd = [{}, {}, {}]\n",
    "        median_daily_psd = [{}, {}, {}]\n",
    "        equal_daily_energy = [{}, {}, {}]\n",
    "    else:\n",
    "        if do_plot:\n",
    "            ax.append(plt.subplot(plotRows, 1, channel_index + 1, sharex=ax[0]))\n",
    "\n",
    "    # Go through all PSDs and reject the 'bad' ones based on the station baseline\n",
    "    # only done when remove_outliers is True.\n",
    "    if remove_outliers:\n",
    "        if verbose:\n",
    "            msgLib.info('CLEAN UP ' + str(len(psd_values)) + ' PSDs')\n",
    "        (ok, notok) = check_y_range(psd_values, pct_low, pct_high)\n",
    "    else:\n",
    "        # No cleanup needed, mark them all as OK!\n",
    "        notok = list()\n",
    "        ok = range(len(psd_values))\n",
    "\n",
    "    info = ' '.join(\n",
    "        ['Channel', channel, str(len(psd_values)), 'PSDs,', str(len(ok)), 'accepted and', str(len(notok)), 'rejected',\n",
    "         '\\n'])\n",
    "    report_header += info\n",
    "    print ('[INFO]', info)\n",
    "\n",
    "    if verbose and notok:\n",
    "        t0 = time_it(t0)\n",
    "        msgLib.info('Flag BAD PSDs')\n",
    "    for i, index in enumerate(ok):\n",
    "        # DAY,DAYTIME: 2018-01-01 2018-01-01T00:00:00.000Z\n",
    "        day = day_values[index]\n",
    "        day_time = day_time_values[index]\n",
    "        psd = psd_values[index]\n",
    "\n",
    "        # Preserve the individual PSDs (day_time)\n",
    "        day_time_psd[channel_index][day_time] = psd\n",
    "\n",
    "        # Group PSDs into daily bins\n",
    "        if day not in daily_psd[channel_index].keys():\n",
    "            daily_psd[channel_index][day] = list()\n",
    "        daily_psd[channel_index][day].append(psd)\n",
    "\n",
    "        # Keep track of individual days\n",
    "        if day_values[index] not in day_values_passed[channel_index]:\n",
    "            day_values_passed[channel_index].append(day)\n",
    "    if verbose and notok:\n",
    "        t0 = time_it(t0)\n",
    "\n",
    "    if do_plot:\n",
    "        # Plot the 'bad' PSDs in gray.\n",
    "        if plot_psd and plot_bad:\n",
    "            msgLib.info('[INFO] Plot {} BAD PSDs'.format(len(notok)))\n",
    "            for i, index in enumerate(notok):\n",
    "                if i == 0:\n",
    "                    plt.semilogx(np.array(x_values), psd_values[index], c='gray', label='Rejected')\n",
    "                else:\n",
    "                    plt.semilogx(np.array(x_values), psd_values[index], c='gray')\n",
    "            if verbose >= 0:\n",
    "                t0 = time_it(t0)\n",
    "\n",
    "        if plot_psd:\n",
    "            # Plot the 'good' PSDs in green.\n",
    "            if verbose:\n",
    "                msgLib.info('[INFO] Plot {} GOOD PSDs'.format(len(ok)))\n",
    "            for i, index in enumerate(ok):\n",
    "                if i == 0:\n",
    "                    plt.semilogx(np.array(x_values), psd_values[index], c='green', label='PSD')\n",
    "                else:\n",
    "                    plt.semilogx(np.array(x_values), psd_values[index], c='green')\n",
    "            if verbose >= 0:\n",
    "                t0 = time_it(t0)\n",
    "        if plot_pdf:\n",
    "            cmap = pqlx\n",
    "\n",
    "            ok = list()\n",
    "            im = plt.scatter(pdf_x, pdf_y, c=pdfP, s=46.5, marker='_', linewidth=param.lw, edgecolor='face', cmap=cmap,\n",
    "                             alpha=param.alpha)\n",
    "            ax[channel_index].set_xscale('log')\n",
    "\n",
    "        if verbose:\n",
    "            msgLib.info('Tune plots.')\n",
    "        if verbose >= 0:\n",
    "            t0 = time_it(t0)\n",
    "        if remove_outliers:\n",
    "            plt.semilogx(np.array(x_values), pct_high, c='yellow', label='{}%'.format(percent_high))\n",
    "            plt.semilogx(np.array(x_values), pct_mid, c='red', label='{}%'.format(percent_mid))\n",
    "            plt.semilogx(np.array(x_values), pct_low, c='orange', label='{}%'.format(percent_low))\n",
    "        plt.semilogx((param.hvsrXlim[0], param.hvsrXlim[0]), param.yLim, c='black')\n",
    "        plt.semilogx((param.hvsrXlim[1], param.hvsrXlim[1]), param.yLim, c='black')\n",
    "        p1 = plt.axvspan(param.xLim[xtype][0], param.hvsrXlim[0], facecolor='#909090', alpha=0.5)\n",
    "        p2 = plt.axvspan(param.hvsrXlim[1], param.xLim[xtype][1], facecolor='#909090', alpha=0.5)\n",
    "        # plt.title(' '.join([target,start,'to',end]))\n",
    "        plt.ylim(param.yLim)\n",
    "        plt.xlim(param.xLim[xtype])\n",
    "        plt.ylabel(param.yLabel)\n",
    "\n",
    "        if len(ok) <= 0:\n",
    "            anchored_text = AnchoredText(\n",
    "                ' '.join(['.'.join([network, station, location, channel]), '{:,d}'.format(len(psd_values)), 'PSDs']),\n",
    "                loc=2)\n",
    "        else:\n",
    "            anchored_text = AnchoredText(' '.join(\n",
    "                ['.'.join([network, station, location, channel]), '{:,d}'.format(len(ok)), 'out of',\n",
    "                 '{:,d}'.format(len(psd_values)), 'PSDs']), loc=2)\n",
    "        ax[channel_index].add_artist(anchored_text)\n",
    "\n",
    "        if plot_nnm:\n",
    "            nlnm_x, nlnm_y = get_nlnm()\n",
    "            nhnm_x, nhnm_y = get_nhnm()\n",
    "            if xtype != 'period':\n",
    "                nlnm_x = 1.0 / nlnm_x\n",
    "                nhnm_x = 1.0 / nhnm_x\n",
    "            plt.plot(nlnm_x, nlnm_y, lw=2, ls='--', c='k', label='NLNM, NHNM')\n",
    "            plt.plot(nhnm_x, nhnm_y, lw=2, ls='--', c='k')\n",
    "\n",
    "        plt.legend(prop={'size': 6}, loc='lower left')\n",
    "\n",
    "        # Create a second axes for the colorbar.\n",
    "        if plot_pdf and ax2 is None:\n",
    "            ax2 = fig.add_axes([0.92, 0.4, 0.01, 0.4])\n",
    "            cbar = fig.colorbar(im, ax2, orientation='vertical')\n",
    "            cbar.set_label('Probability (%)', size=9, rotation=270, labelpad=6)\n",
    "            plt.clim(param.pMin, param.pMax)\n",
    "\n",
    "\n",
    "    # Compute and save the median daily PSD for HVSR computation\n",
    "    # for non-DFA computation.\n",
    "    # daily_psd[channel_index][day] is a list of individual PSDs for that channel and day. We compute median\n",
    "    # along axis=0 to get median of individual frequencies.\n",
    "    if not dfa:\n",
    "        if verbose:\n",
    "            msgLib.info('Save Median Daily')\n",
    "        for day in (day_values_passed[channel_index]):\n",
    "            if display:\n",
    "                print('[INFO] calculating median_daily_psd', flush=True)\n",
    "                display = False\n",
    "            median_daily_psd[channel_index][day] = np.percentile(daily_psd[channel_index][day], 50, axis=0)\n",
    "\n",
    "# Are we doing DFA?\n",
    "# Use equal energy for daily PSDs to give small 'events' a chance to contribute\n",
    "# the same as large ones, so that P1+P2+P3=1\n",
    "if dfa:\n",
    "    if display:\n",
    "        print('[INFO] DFA', flush=True)\n",
    "        display = False\n",
    "    sum_ns_power = list()\n",
    "    sum_ew_power = list()\n",
    "    sum_z_power = list()\n",
    "    daily_psd = [{}, {}, {}]\n",
    "    day_values = list()\n",
    "\n",
    "    # Make sure we have all 3 components for every time sample\n",
    "    for day_time in day_time_values:\n",
    "        if day_time not in (day_time_psd[0].keys()) or day_time not in (day_time_psd[1].keys()) or day_time not in (\n",
    "        day_time_psd[2].keys()):\n",
    "            continue\n",
    "        day = day_time.split('T')[0]\n",
    "        if day not in day_values:\n",
    "            day_values.append(day)\n",
    "\n",
    "        # Initialize the daily PSDs.\n",
    "        if day not in daily_psd[0].keys():\n",
    "            daily_psd[0][day] = list()\n",
    "            daily_psd[1][day] = list()\n",
    "            daily_psd[2][day] = list()\n",
    "\n",
    "        daily_psd[0][day].append(day_time_psd[0][day_time])\n",
    "        daily_psd[1][day].append(day_time_psd[1][day_time])\n",
    "        daily_psd[2][day].append(day_time_psd[2][day_time])\n",
    "\n",
    "    # For each day equalize energy\n",
    "    for day in day_values:\n",
    "\n",
    "        # Each PSD for the day\n",
    "        for i in range(len(daily_psd[0][day])):\n",
    "            Pz = list()\n",
    "            P1 = list()\n",
    "            P2 = list()\n",
    "            sum_pz = 0\n",
    "            sum_p1 = 0\n",
    "            sum_p2 = 0\n",
    "\n",
    "            # Each sample of the PSD , convert to power\n",
    "            for j in range(len(x_values) - 1):\n",
    "                pz = get_power([daily_psd[0][day][i][j], daily_psd[0][day][i][j + 1]], [x_values[j], x_values[j + 1]])\n",
    "                Pz.append(pz)\n",
    "                sum_pz += pz\n",
    "                p1 = get_power([daily_psd[1][day][i][j], daily_psd[1][day][i][j + 1]], [x_values[j], x_values[j + 1]])\n",
    "                P1.append(p1)\n",
    "                sum_p1 += p1\n",
    "                p2 = get_power([daily_psd[2][day][i][j], daily_psd[2][day][i][j + 1]], [x_values[j], x_values[j + 1]])\n",
    "                P2.append(p2)\n",
    "                sum_p2 += p2\n",
    "\n",
    "            sum_power = sum_pz + sum_p1 + sum_p2  # total power\n",
    "\n",
    "            # Mormalized power\n",
    "            for j in range(len(x_values) - 1):\n",
    "                # Initialize if this is the first sample of the day\n",
    "                if i == 0:\n",
    "                    sum_z_power.append(Pz[j] / sum_power)\n",
    "                    sum_ns_power.append(P1[j] / sum_power)\n",
    "                    sum_ew_power.append(P2[j] / sum_power)\n",
    "                else:\n",
    "                    sum_z_power[j] += (Pz[j] / sum_power)\n",
    "                    sum_ns_power[j] += (P1[j] / sum_power)\n",
    "                    sum_ew_power[j] += (P2[j] / sum_power)\n",
    "        # Average the normalized daily power\n",
    "        for j in range(len(x_values) - 1):\n",
    "            sum_z_power[j] /= len(daily_psd[0][day])\n",
    "            sum_ns_power[j] /= len(daily_psd[0][day])\n",
    "            sum_ew_power[j] /= len(daily_psd[0][day])\n",
    "\n",
    "        equal_daily_energy[0][day] = sum_z_power\n",
    "        equal_daily_energy[1][day] = sum_ns_power\n",
    "        equal_daily_energy[2][day] = sum_ew_power\n",
    "\n",
    "# HVSR computation\n",
    "if verbose:\n",
    "    msgLib.info('HVSR computation')\n",
    "if verbose >= 0:\n",
    "    t0 = time_it(t0)\n",
    "\n",
    "# Find the unique days between all channels\n",
    "d = day_values_passed[0]\n",
    "for i in range(1, len(day_values_passed)):\n",
    "    d += day_values_passed[i]\n",
    "day_values_passed = set(d)  # unique days\n",
    "\n",
    "hvsr = list()\n",
    "peak_water_level = list()\n",
    "hvsrp = list()\n",
    "peak_water_level_p = list()\n",
    "hvsrp2 = list()\n",
    "hvsrm = list()\n",
    "water_level_m = list()\n",
    "peak_water_level_m = list()\n",
    "hvsr_m2 = list()\n",
    "hvsr_std = list()\n",
    "hvsr_log_std = list()\n",
    "\n",
    "path = ''.join(['M', str(method)])\n",
    "fileLib.mkdir(param.hvsrDirectory, path)\n",
    "out_file_name = os.path.join(param.hvsrDirectory, path, fileLib.hvsrFileName(network, station, location, start, end))\n",
    "\n",
    "outFile = open(out_file_name, 'w')\n",
    "msgLib.info(f'Output file: {out_file_name}')\n",
    "count = -1\n",
    "\n",
    "# compute one x-value (period or frequency) at a time to also compute standard deviation\n",
    "outFile.write('frequency HVSR HVSR+1STD HVSR-1STD\\n')\n",
    "for j in range(len(x_values) - 1):\n",
    "    missing = 0\n",
    "    hvsr_tmp = list()\n",
    "\n",
    "    for day in sorted(day_values_passed):\n",
    "\n",
    "        # must have all 3 channels, compute HVSR for that day\n",
    "        if dfa:\n",
    "            if day in equal_daily_energy[0].keys() and day in equal_daily_energy[1].keys() and day in \\\n",
    "                    equal_daily_energy[2].keys():\n",
    "                hvsr0 = math.sqrt(\n",
    "                    (equal_daily_energy[1][day][j] + equal_daily_energy[2][day][j]) / equal_daily_energy[0][day][j])\n",
    "                hvsr_tmp.append(hvsr0)\n",
    "            else:\n",
    "                if verbose > 0:\n",
    "                    msgLib.warning(sys.argv[0], day + ' missing component, skipped!')\n",
    "                missing += 1\n",
    "                continue\n",
    "        else:\n",
    "            if day in median_daily_psd[0].keys() and day in median_daily_psd[1].keys() and day in \\\n",
    "                    median_daily_psd[2].keys():\n",
    "                psd0 = [median_daily_psd[0][day][j], median_daily_psd[0][day][j + 1]]\n",
    "                psd1 = [median_daily_psd[1][day][j], median_daily_psd[1][day][j + 1]]\n",
    "                psd2 = [median_daily_psd[2][day][j], median_daily_psd[2][day][j + 1]]\n",
    "                hvsr0 = get_hvsr(psd0, psd1, psd2, [x_values[j], x_values[j + 1]], use_method=method)\n",
    "                hvsr_tmp.append(hvsr0)\n",
    "            else:\n",
    "                if verbose > 0:\n",
    "                    msgLib.warning(sys.argv[0], day + ' missing component, skipped!')\n",
    "                missing += 1\n",
    "                continue\n",
    "    count += 1\n",
    "    peak_water_level.append(water_level)\n",
    "    if len(hvsr_tmp) > 0:\n",
    "        hvsr.append(np.mean(hvsr_tmp))\n",
    "        hvsr_std.append(np.std(hvsr_tmp))\n",
    "        hvsr_log_std.append(np.std(np.log10(hvsr_tmp)))\n",
    "        hvsrp.append(hvsr[-1] + hvsr_std[-1])\n",
    "        peak_water_level_p.append(water_level + hvsr_std[-1])\n",
    "        hvsrp2.append(hvsr[-1] * math.exp(hvsr_log_std[count]))\n",
    "        hvsrm.append(hvsr[-1] - hvsr_std[-1])\n",
    "        peak_water_level_m.append(water_level - hvsr_std[-1])\n",
    "        hvsr_m2.append(hvsr[-1] / math.exp(hvsr_log_std[-1]))\n",
    "        # outFile.write(str(x_values[count])+'  '+str(hvsr[-1])+'  '+str(hvsrp[-1])+'  '+str(hvsrm[-1])+'\\n')\n",
    "        outFile.write(\n",
    "            '%s %0.3f %0.3f %0.3f\\n' % (str(x_values[count]), float(hvsr[-1]), float(hvsrp[-1]), float(str(hvsrm[-1]))))\n",
    "outFile.close()\n",
    "\n",
    "# Compute day at a time to also compute frequency standard deviation\n",
    "missing = 0\n",
    "\n",
    "# This holds the peaks for individual HVSRs that will contribute to the final HVSR. It will be used to find Sigmaf.\n",
    "hvsrPeaks = list()\n",
    "\n",
    "for day in sorted(day_values_passed):\n",
    "    hvsr_tmp = list()\n",
    "    for j in range(len(x_values) - 1):\n",
    "        if dfa > 0:\n",
    "            if day in equal_daily_energy[0].keys() and day in equal_daily_energy[1].keys() and day in \\\n",
    "                    equal_daily_energy[2].keys():\n",
    "                hvsr0 = math.sqrt(\n",
    "                    (equal_daily_energy[1][day][j] + equal_daily_energy[2][day][j]) / equal_daily_energy[0][day][j])\n",
    "                hvsr_tmp.append(hvsr0)\n",
    "            else:\n",
    "                if verbose > 0:\n",
    "                    msgLib.warning(sys.argv[0], day + ' missing component, skipped!')\n",
    "                missing += 1\n",
    "                continue\n",
    "        else:\n",
    "            if day in median_daily_psd[0].keys() and day in median_daily_psd[1].keys() and day in \\\n",
    "                    median_daily_psd[2].keys():\n",
    "                psd0 = [median_daily_psd[0][day][j], median_daily_psd[0][day][j + 1]]\n",
    "                psd1 = [median_daily_psd[1][day][j], median_daily_psd[1][day][j + 1]]\n",
    "                psd2 = [median_daily_psd[2][day][j], median_daily_psd[2][day][j + 1]]\n",
    "                hvsr0 = get_hvsr(psd0, psd1, psd2, [x_values[j], x_values[j + 1]], use_method=method)\n",
    "                hvsr_tmp.append(hvsr0)\n",
    "            else:\n",
    "                if verbose > 0:\n",
    "                    msgLib.warning(sys.argv[0], day + ' missing component, skipped!')\n",
    "                missing += 1\n",
    "                continue\n",
    "    if not np.isnan(np.sum(hvsr_tmp)):\n",
    "        hvsrPeaks.append(find_peaks(hvsr_tmp))\n",
    "\n",
    "report_header += '\\n'\n",
    "report_header += ' '.join([str(missing), 'PSDs are missing one or more components\\n'])\n",
    "\n",
    "# Find  the relative extrema of hvsr\n",
    "if not np.isnan(np.sum(hvsr)):\n",
    "    indexList = find_peaks(hvsr)\n",
    "else:\n",
    "    indexList = list()\n",
    "\n",
    "stdf = list()\n",
    "for index in indexList:\n",
    "    point = list()\n",
    "    for j in range(len(hvsrPeaks)):\n",
    "        p = None\n",
    "        for k in range(len(hvsrPeaks[j])):\n",
    "            if p is None:\n",
    "                p = hvsrPeaks[j][k]\n",
    "            else:\n",
    "                if abs(index - hvsrPeaks[j][k]) < abs(index - p):\n",
    "                    p = hvsrPeaks[j][k]\n",
    "        if p is not None:\n",
    "            point.append(p)\n",
    "    point.append(index)\n",
    "    v = list()\n",
    "    for l in range(len(point)):\n",
    "        v.append(x_values[point[l]])\n",
    "    stdf.append(np.std(v))\n",
    "\n",
    "peak = init_peaks(x_values, hvsr, indexList, hvsr_band, peak_water_level)\n",
    "peak = check_clarity(x_values, hvsr, peak, True)\n",
    "\n",
    "# Find  the relative extrema of hvsrp (hvsr + 1 standard deviation)\n",
    "if not np.isnan(np.sum(hvsrp)):\n",
    "    index_p = find_peaks(hvsrp)\n",
    "else:\n",
    "    index_p = list()\n",
    "\n",
    "peakp = init_peaks(x_values, hvsrp, index_p, hvsr_band, peak_water_level_p)\n",
    "peakp = check_clarity(x_values, hvsrp, peakp)\n",
    "\n",
    "# Find  the relative extrema of hvsrp (hvsr - 1 standard deviation)\n",
    "if not np.isnan(np.sum(hvsrm)):\n",
    "    index_m = find_peaks(hvsrm)\n",
    "else:\n",
    "    index_m = list()\n",
    "\n",
    "peakm = init_peaks(x_values, hvsrm, index_m, hvsr_band, peak_water_level_m)\n",
    "peakm = check_clarity(x_values, hvsrm, peakm)\n",
    "\n",
    "peak = check_stability(stdf, peak, hvsr_log_std, True)\n",
    "peak = check_freq_stability(peak, peakm, peakp)\n",
    "\n",
    "if do_plot > 0 and len(hvsr) > 0:\n",
    "    nx = len(x_values) - 1\n",
    "    plt.suptitle(plot_title)\n",
    "    if plot_pdf or plot_psd:\n",
    "        ax.append(plt.subplot(plotRows, 1, 4))\n",
    "    else:\n",
    "        ax.append(plt.subplot(1, 1, 1))\n",
    "\n",
    "    plt.semilogx(np.array(x_values[0:nx]), hvsr, lw=1, c='blue', label='HVSR')\n",
    "    plt.semilogx(np.array(x_values[0:nx]), hvsrp, c='red', lw=1, ls='--',\n",
    "                 label='{} {}'.format(get_char(u'\\u00B11'), get_char(u'\\u03C3')))\n",
    "    plt.semilogx(np.array(x_values[0:nx]), hvsrm, c='red', lw=1, ls='--')\n",
    "    # plt.semilogx(np.array(x_values),hvsrp2,c='r',lw=1,ls='--')\n",
    "    # plt.semilogx(np.array(x_values),hvsr_m2,c='r',lw=1,ls='--')\n",
    "    plt.ylabel(param.hvsrYlabel)\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    plt.xlim(param.hvsrXlim)\n",
    "    ax[-1].set_ylim(hvsr_ylim)\n",
    "    plt.xlabel(param.xLabel[xtype])\n",
    "\n",
    "    for i in range(len(peak)):\n",
    "        plt.semilogx(peak[i]['f0'], peak[i]['A0'], marker='o', c='r')\n",
    "        plt.semilogx((peak[i]['f0'], peak[i]['f0']), (hvsr_ylim[0], peak[i]['A0']), c='red')\n",
    "        if stdf[i] < float(peak[i]['f0']):\n",
    "            dz = stdf[i]\n",
    "            plt.axvspan(float(peak[i]['f0']) - dz, float(peak[i]['f0']) + dz, facecolor='#909090', alpha=0.5)\n",
    "            plt.semilogx((peak[i]['f0'], peak[i]['f0']), (hvsr_ylim[0], hvsr_ylim[1]), c='#dcdcdc', lw=0.5)\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(param.imageDirectory + '/' + fileLib.hvsrFileName(network, station, location, start, end)).replace(\n",
    "            '.txt', '.png'), dpi=param.imageDpi, transparent=True, bbox_inches='tight', pad_inches=0.1)\n",
    "if not dfa:\n",
    "    print_peak_report(station_header, report_header, peak, report_information, min_rank)\n",
    "\n",
    "if show_plot:\n",
    "    if verbose >= 0:\n",
    "        msgLib.info('SHOW PLOT')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef135556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/birotimi/Library/CloudStorage/OneDrive-SyracuseUniversity/Desktop/HVSR-master'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7994777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c216c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Users/birotimi/Library/CloudStorage/OneDrive-SyracuseUniversity/Syracuse 2014-05-08/hvsr/M1/TA.J57A.--.2014-05-08.2014-05-08.HVSR.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUsers/birotimi/Library/CloudStorage/OneDrive-SyracuseUniversity/Syracuse 2014-05-08/hvsr/M1/TA.J57A.--.2014-05-08.2014-05-08.HVSR.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Users/birotimi/Library/CloudStorage/OneDrive-SyracuseUniversity/Syracuse 2014-05-08/hvsr/M1/TA.J57A.--.2014-05-08.2014-05-08.HVSR.txt'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Users/birotimi/Library/CloudStorage/OneDrive-SyracuseUniversity/Syracuse 2014-05-08/hvsr/M1/TA.J57A.--.2014-05-08.2014-05-08.HVSR.txt', sep = \" \", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a51f1071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/birotimi\n"
     ]
    }
   ],
   "source": [
    "cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0f680f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/birotimi'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c843ead",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
